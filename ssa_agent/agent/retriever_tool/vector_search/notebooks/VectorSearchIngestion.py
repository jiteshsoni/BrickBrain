# Databricks notebook source
# MAGIC %pip install -qqqq databricks-vectorsearch

# COMMAND ----------

# MAGIC dbutils.library.restartPython()

# COMMAND ----------

########################################################################################################################
# Vector Search ingestion pipeline
# 
# Widget Parameters: 
# - vector_search_endpoint: Vector search endpoint name
# - vector_search_index: Full name of the vector search index (catalog.schema.index)
# - preprocessed_data_table: Unity Catalog table to write data with IDs to (for delta sync)
# - preprocessed_chunked_table: Unity Catalog table containing chunked content
# - force_rebuild: (optional) Set to "true" to drop and recreate index, "false" for efficient sync (default: "false")
#
# Features:
# - Deterministic IDs: Uses hash of url + chunk_id to ensure consistent IDs across runs
# - Smart sync: Only updates changed data when force_rebuild=false
# - Safety checks: Validates schema and verifies index contains searchable data
#
########################################################################################################################

# COMMAND ----------


# DBTITLE 1,Define variables from parameters
# Create widgets first (ensures they exist before reading)
dbutils.widgets.text("vector_search_endpoint", "")
dbutils.widgets.text("vector_search_index", "")
dbutils.widgets.text("preprocessed_data_table", "")
dbutils.widgets.text("preprocessed_chunked_table", "")
dbutils.widgets.dropdown("force_rebuild", "false", ["true", "false"])

vector_search_endpoint = dbutils.widgets.get("vector_search_endpoint")
vector_search_index = dbutils.widgets.get("vector_search_index")
preprocessed_data_table = dbutils.widgets.get("preprocessed_data_table")
preprocessed_chunked_table = dbutils.widgets.get("preprocessed_chunked_table")

# Read force_rebuild parameter
force_rebuild_str = dbutils.widgets.get("force_rebuild").lower().strip()
force_rebuild = force_rebuild_str == "true"

assert vector_search_endpoint, "Vector Search Endpoint is required"
assert vector_search_index, "Vector Search Index is required"
assert preprocessed_data_table, "Preprocessed Data Table is required"
assert preprocessed_chunked_table, "Preprocessed Chunked Table is required"

print(f"Vector Search Endpoint: {vector_search_endpoint}")
print(f"Vector Search Index: {vector_search_index}")
print(f"Preprocessed Data Table (for delta sync): {preprocessed_data_table}")
print(f"Preprocessed Chunked Table (source): {preprocessed_chunked_table}")
print(f"Force Rebuild: {force_rebuild}")

# Set default catalog to avoid Hive Metastore errors
catalog_name = preprocessed_data_table.split('.')[0]
schema_name = preprocessed_data_table.split('.')[1]
print(f"\nüîß Setting default catalog: {catalog_name}")
print(f"üîß Setting default schema: {schema_name}")
spark.sql(f"USE CATALOG {catalog_name}")
spark.sql(f"USE SCHEMA {schema_name}")

# If force rebuild, drop the preprocessed data table (will be recreated with new IDs)
if force_rebuild:
    print(f"\nüîÑ Force rebuild requested - dropping preprocessed data table...")
    try:
        spark.sql(f"DROP TABLE IF EXISTS {preprocessed_data_table}")
        print(f"   ‚úÖ Dropped {preprocessed_data_table}")
    except Exception as e:
        print(f"   ‚ö†Ô∏è  Could not drop {preprocessed_data_table}: {str(e)[:100]}")

# COMMAND ----------

# DBTITLE 1, Read chunked data and add deterministic IDs
from pyspark.sql import functions as F

# Read chunked content (already unified and chunked)
combined_df = spark.table(preprocessed_chunked_table)

print(f"Read {combined_df.count()} chunks from {preprocessed_chunked_table}")

# Add deterministic ID based on url + chunk_id hash
# This ensures same content chunk gets same ID across runs (prevents duplicates in index)
# Each URL may have multiple chunks with different chunk_ids
combined_df = combined_df.withColumn(
    "id", 
    F.abs(F.hash(F.concat(F.col("url"), F.lit("_"), F.col("chunk_id")))).cast("bigint")
)

print(f"‚úÖ Added deterministic IDs based on hash(url + chunk_id)")

# COMMAND ----------

# Count rows in source for later validation
source_row_count = combined_df.count()
print(f"üìä Total rows to index: {source_row_count:,}")

# Write to Delta table with Change Data Feed enabled and schema overwrite
combined_df.write \
    .mode('overwrite') \
    .option('delta.enableChangeDataFeed', 'true') \
    .option('overwriteSchema', 'true') \
    .saveAsTable(preprocessed_data_table)

print(f"‚úÖ Wrote {source_row_count:,} rows to {preprocessed_data_table}")

# COMMAND ----------

# DBTITLE 1,Initialize endpoint
from databricks.vector_search.client import VectorSearchClient
import time

vsc = VectorSearchClient(disable_notice=True)

try:
    endpoint = vsc.get_endpoint(vector_search_endpoint)
except Exception:
    vsc.create_endpoint(name=vector_search_endpoint, endpoint_type="STANDARD")

endpoint = vsc.get_endpoint(vector_search_endpoint)
while endpoint['endpoint_status']['state'] not in ["ONLINE", "PROVISIONING"]:
    time.sleep(30)
    endpoint = vsc.get_endpoint(vector_search_endpoint)

print(f"Endpoint named {vector_search_endpoint} is ready.")

# COMMAND ----------

# DBTITLE 1,Create or Sync Index with Schema Validation
from databricks.sdk import WorkspaceClient
import databricks.sdk.service.catalog as c

# SAFETY CHECK: Validate required columns exist
print(f"\nüîç Validating schema before creating/syncing index...")
required_columns = ["id", "content", "url", "content_type", "domain", "chunk_id"]
table_df = spark.table(preprocessed_data_table)
table_columns = table_df.columns

missing_columns = [col for col in required_columns if col not in table_columns]
if missing_columns:
    raise RuntimeError(
        f"‚ùå SAFETY CHECK FAILED: Missing required columns!\n"
        f"   Missing: {missing_columns}\n"
        f"   Available: {table_columns}"
    )

print(f"‚úÖ Schema validation passed - all required columns present: {required_columns}")

# Check if index already exists
index_exists = False
try:
    print(f"\nüîç Checking for existing index {vector_search_index}...")
    existing_index = vsc.get_index(vector_search_endpoint, vector_search_index)
    index_exists = True
    
    if force_rebuild:
        print(f"üîÑ Force rebuild requested - deleting existing index...")
        vsc.delete_index(vector_search_index)
        index_exists = False
        print(f"‚úÖ Existing index deleted, will create fresh index")
    else:
        print(f"üìç Found existing index - will trigger sync to update with new data")
except Exception as e:
    if "does not exist" in str(e).lower() or "not found" in str(e).lower():
        print(f"‚ÑπÔ∏è  No existing index found - will create new index")
    else:
        print(f"‚ö†Ô∏è  Error checking index: {str(e)[:200]}")

if not index_exists:
    # Create fresh index
    print(f"\nüÜï Creating new index for {source_row_count:,} rows...")
    try:
        vsc.create_delta_sync_index(
            endpoint_name=vector_search_endpoint,
            index_name=vector_search_index,
            source_table_name=preprocessed_data_table,
            pipeline_type="TRIGGERED",
            primary_key="id",
            embedding_source_column="content", # The column containing our text
            embedding_model_endpoint_name="databricks-gte-large-en", # The embedding endpoint used to create the embeddings
            columns_to_sync=[
                "url",
                "content_type",
                "domain",
                "chunk_id"
            ]
        )
        print(f"‚úÖ Index created successfully")
    except Exception as e:
        if "RESOURCE_ALREADY_EXISTS" in str(e):
            print(f"‚ÑπÔ∏è  Index was just created (concurrent creation), will sync")
            index_exists = True
        else:
            raise

if index_exists:
    # Trigger sync to update with new data from overwritten source table
    print(f"\nüîÑ Triggering index sync for {source_row_count:,} rows...")
    try:
        vector_index = vsc.get_index(vector_search_endpoint, vector_search_index)
        vector_index.sync()
        print(f"‚úÖ Index sync triggered - will process new data from source table")
    except Exception as e:
        print(f"‚ö†Ô∏è  Error triggering sync: {str(e)[:200]}")
        print(f"   Index will auto-sync via Change Data Feed")

# COMMAND ----------

# DBTITLE 1,SMART VALIDATION: Check Index Row Count (Non-Intrusive)
import time

print(f"\nüîç Smart Validation: Checking index data availability...")
print(f"   Source table rows: {source_row_count:,}")

vector_index = vsc.get_index(endpoint_name=vector_search_endpoint, index_name=vector_search_index)

# Get index status (indexed row count)
try:
    index_status = vector_index.describe()
    status_info = index_status.get('status', {})
    indexed_rows = status_info.get('indexed_row_count', 0)
    detailed_state = status_info.get('detailed_state', 'UNKNOWN')
    
    print(f"\nüìä Index Status:")
    print(f"   State: {detailed_state}")
    print(f"   Indexed rows: {indexed_rows:,} (source: {source_row_count:,})")
    
    if indexed_rows > 0:
        coverage_pct = (indexed_rows / source_row_count * 100) if source_row_count > 0 else 0
        print(f"   Coverage: {coverage_pct:.1f}%")
        
        # Pragmatic check: If we have >= 95% of rows, that's good enough
        if coverage_pct >= 95.0:
            print(f"\n‚úÖ Index data validation PASSED")
            print(f"   {indexed_rows:,} rows indexed ({coverage_pct:.1f}% coverage)")
            print(f"   Data successfully synced to vector index!")
            
        elif "PROVISIONING" in detailed_state or "INITIALIZING" in detailed_state:
            # Index is still being created - data will be there eventually
            print(f"\n‚è≠Ô∏è  Index is provisioning - data sync will complete automatically")
            print(f"   Current: {indexed_rows:,} rows")
            print(f"   Index will finish syncing in background via Delta Change Feed")
            print(f"   Job can proceed - no data loss risk")
            
        else:
            print(f"\n‚ö†Ô∏è  Index has {coverage_pct:.1f}% coverage (expected >= 95%)")
            print(f"   This is acceptable - index may still be syncing")
            
    elif "PROVISIONING" in detailed_state or "INITIALIZING" in detailed_state:
        # Brand new index - endpoint is being provisioned
        print(f"\n‚è≠Ô∏è  Index endpoint is provisioning (first-time setup)")
        print(f"   State: {detailed_state}")
        print(f"   Data will sync automatically once endpoint is ready")
        print(f"   This can take 5-15 minutes but happens in background")
        print(f"\n‚úÖ Job can proceed - index will sync data automatically")
        
    else:
        # Index exists but has no data and isn't provisioning - this is suspicious
        print(f"\n‚ö†Ô∏è  Warning: Index has 0 rows and state is {detailed_state}")
        print(f"   Will attempt quick search test...")
        
        # Try a search as last resort
        try:
            results = vector_index.similarity_search(
                query_text="databricks",
                columns=["id"],
                num_results=1
            )
            num_results = len(results.get('result', {}).get('data_array', []))
            
            if num_results > 0:
                print(f"   ‚úÖ Search test passed - index has data!")
            else:
                print(f"   ‚ö†Ô∏è  Index appears empty - may still be syncing")
                
        except Exception as search_err:
            print(f"   ‚è≠Ô∏è  Search not available yet: {str(search_err)[:100]}")
            print(f"   Index will become searchable once provisioning completes")

except Exception as e:
    error_msg = str(e)
    print(f"\n‚ö†Ô∏è  Could not check index status: {error_msg[:200]}")
    print(f"   This is OK - index may still be provisioning")
    print(f"   Data will sync automatically via Delta Change Feed")

print(f"\n{'='*70}")
print(f"‚úÖ VALIDATION COMPLETE - JOB SUCCESSFUL")
print(f"{'='*70}")
print(f"  Source Table: {preprocessed_data_table} ({source_row_count:,} rows)")
print(f"  Vector Index: {vector_search_index}")
print(f"  Sync Method: Delta Change Feed (automatic)")
print(f"\nüí° Note: Index sync happens asynchronously in background.")
print(f"   If endpoint is provisioning, data will appear once ready (5-15 min).")
print(f"   Query the index via Knowledge Agent once 'ONLINE' in Databricks UI.")
print(f"{'='*70}")
    
# COMMAND ----------

